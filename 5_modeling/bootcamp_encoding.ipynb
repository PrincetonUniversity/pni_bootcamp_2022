{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c22893-cae9-4c7d-9b87-74a96e89c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9b513-fd5d-4616-96b1-6673a426b94e",
   "metadata": {},
   "source": [
    "## Bernoulli Coin Flips\n",
    "Write a function that takes a list of 0s and 1s that represent a series of coin tosses, with 1s meaning heads and 0s meaning tails.\n",
    "Have your function output the log likelihood of observing the list of results given they were generated using a fair coin.\n",
    "\n",
    "Recall that the equation for the PMF of this type of random variable is $ f(x) = p^x(1-p)^{(1-x)} $\n",
    "This tells you the probability of observing a value of x (which is 0 or 1). To find the joint probability of a set of coin flips, you can multiply the probability of each flip together.\n",
    "We want the log-likelihood, and remember that logs turn products into sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49771e-31ed-4177-9219-1cf411754ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_LL(coin_tosses):\n",
    "    # fill in function here\n",
    "    \n",
    "    return LL\n",
    "\n",
    "\n",
    "num_flips = 10\n",
    "coin_tosses = np.random.choice([0,1], size=num_flips)\n",
    "print(bernoulli_LL(coin_tosses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd99e19-ca8f-4e39-9370-c9c4483a1232",
   "metadata": {},
   "source": [
    "Modify your function (copy-paste it below) to also take a value between 0 and 1 as an argument. Your new function should treat this value as the probability of flipping heads and return the appropriate log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7d58d-9574-41eb-b8d3-f1411701aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_LL(coin_tosses, p_heads):\n",
    "    # fill in function here\n",
    "    \n",
    "    return LL\n",
    "\n",
    "print(bernoulli_LL(coin_tosses, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5956164-2547-47fd-98c5-d3dad9213d93",
   "metadata": {},
   "source": [
    "Write a function that takes a series of coin tosses as before, but returns the probability of flipping heads that is most likely by maximizing the log-likelihood. To find the maximum, use the 1st derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab3869-c1bd-4968-b28e-60aa71188de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bernoulli(coin_tosses):\n",
    "    # fill in function here\n",
    "\n",
    "    return p\n",
    "\n",
    "num_flips = 10\n",
    "coin_tosses = np.random.choice([0,1], size=num_flips)\n",
    "best_p = fit_bernoulli(coin_tosses)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8862c66-bac8-4205-97ff-8eb52ff41128",
   "metadata": {},
   "source": [
    "What value do you think you will approach as you increase num_flips? What about in the code below? See documentation for [numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa7c0c-74cd-4caf-bdd3-3c1a3ecdc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_flips = 10\n",
    "# now lets tell random.choice to use a specific p_heads\n",
    "coin_tosses = np.random.choice([0,1], size=num_flips, p=[0.58,0.42])\n",
    "best_p = fit_bernoulli(coin_tosses)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26589ef8-4e75-4bdb-8032-a07e532d96c3",
   "metadata": {},
   "source": [
    "This is a direct demonstration of how increasing your statistical power can help you develop encoding models that are more likely to align with the true encoding scheme. Now let's see how random noise is effecting things. We will try gradually increasing num_flips and generating several sets of coin tosses at each value. Then we will plot the parameters of the bernoulli distributions we fit each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95ccef-be0a-4d5c-9a87-d80dc96a8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1122)\n",
    "# let's try log-spaced flip quantities\n",
    "num_flips = [10,100,1000,10000,100000]\n",
    "fit_probabilities = np.ndarray((len(num_flips),10))\n",
    "for nf_idx, nf in enumerate(num_flips):\n",
    "    # let's get 10 samples each time\n",
    "    for ct in np.arange(10):\n",
    "        # each iterative call of random.choice will create a new set of coin tosses\n",
    "        # this serves as our \"noise\" in this example\n",
    "        coin_tosses = np.random.choice([0,1], size=nf)\n",
    "        fit_probabilities[nf_idx,ct] = fit_bernoulli(coin_tosses)\n",
    "\n",
    "# plot results\n",
    "fit_means = np.mean(fit_probabilities, axis=1)\n",
    "fit_stds = np.std(fit_probabilities, axis=1)\n",
    "plt.plot(num_flips, fit_means, c='k')\n",
    "plt.fill_between(num_flips, fit_means+fit_stds, fit_means-fit_stds, alpha=.5, color='b')\n",
    "plt.gca().set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aed9e2-6df0-4a4e-ba5c-e566406df6db",
   "metadata": {},
   "source": [
    "Not only do we converge on the true value, but we are much more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b00d9b-017d-4c14-bb90-b0e5b89db13f",
   "metadata": {},
   "source": [
    "## Gaussian Spiking Neuron\n",
    "Now let's apply the same basic concept to modeling a spiking neuron as a linear-gaussian function of a binary stimulus. This is equivalent to using ordinary least squares regression, and is also a kind of GLM where our \"nonlinearity\" is linear and our noise distribution is a gaussian.\n",
    "\n",
    "First let's load the data and see what it looks like. This data is from a single retinal ganglion cell being shown a binary gaussian white noise visual input (what was shown in the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd147697-221e-4c4b-af7a-7aa73e55eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the RGC3.npz file is in your working directory, or provide the full path\n",
    "data = np.load('RGC3.npz')['args']\n",
    "stim = data[0] # the value of the stimulus at each timepoint\n",
    "spikes_binned = data[1] # the number of spikes in each time bin. We have 1 bin per frame of stimulus\n",
    "stim_times = data[2] # how long in absolute time has elapsed at each timepoint\n",
    "dt_stim = stim_times[1] - stim_times[0] # the bin size in absolute time\n",
    "\n",
    "# Let's visualize some of the raw data\n",
    "fig, (ax1,ax2) = plt.subplots(2)\n",
    "fig.set_size_inches(12,8)\n",
    "iiplot = np.arange(120)\n",
    "ttplot = iiplot*dt_stim\n",
    "ax1.plot(ttplot, stim[iiplot])\n",
    "ax1.set_title('raw stimulus')\n",
    "ax1.set_ylabel('stim intensity')\n",
    "ax1.set_xlim([ttplot[0],ttplot[-1]])\n",
    "\n",
    "ax2.stem(ttplot,spikes_binned[iiplot])\n",
    "ax2.set_xlim([ttplot[0], ttplot[-1]])\n",
    "ax2.set_xlabel('time (s)')\n",
    "ax2.set_title('binned spike counts')\n",
    "ax2.set_ylabel('spike count')\n",
    "ax2.set_xlim([ttplot[0],ttplot[-1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('------------------------------------------------')\n",
    "print(f'Our time bin size is about {dt_stim*1000:.1f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5b986-d2b5-4077-a104-a04d731ce7aa",
   "metadata": {},
   "source": [
    "Now we need to make our design matrix so that we can conveniently use the dot product to keep N frames of the stimulus preceeding each spike. Basically we need a matrix in which each row contains N frames prior to each timepoint. This creates a problem for the first N timepoints, where there are some frames of stimulus preceeding them that don't exist. Since our stimulus varies between 0.48 and -0.48, we can just pretend there were stimulus frames set to 0, we can \"pad\" our stimulus with 0s.\n",
    "\n",
    "Now create the design matrix where N is 25. It should look like the matrix that was in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac4f09-03a1-4c74-9b7e-0e633394b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=25 # use the last 10 frames\n",
    "padded_stim = np.hstack((np.zeros((N-1)), stim)) # pad with 0s\n",
    "\n",
    "# insert code here, you will need to do some looping\n",
    "\n",
    "X = \n",
    "\n",
    "# The design matrix is huge, but we can just look at the beginning of it to make sure it looks right\n",
    "# it should look similar to the one in the slides.\n",
    "plt.clf()\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.imshow(X[:50], aspect='auto', interpolation='none')\n",
    "plt.xlabel('lags before spike time')\n",
    "plt.ylabel('time bin of response')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6f8d0-009b-4b25-b3a8-68afe1de0f11",
   "metadata": {},
   "source": [
    "Now let's see what our encoding model finds under the assumptions that it is linear and distributed as a gaussian. Remember we can just use the least squares formula. This is the same thing as the spike-triggered average (STA).\n",
    "$(X^TX)^{-1}X^TY$\n",
    "\n",
    "We don't even need to include the stimulus covariance part, because our stimulus is gaussian white noise, but we will anyway.\n",
    "Recall that in python, the dot product is @, and you can use [numpy.linalg.inv](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) to do the inverse or [numpy.linalg.solve](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d5626-6506-48d6-9201-78f1de21b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here\n",
    "\n",
    "thetas = \n",
    "\n",
    "# insert plotting code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f1900-156e-4ae3-be41-bd77dd6506f8",
   "metadata": {},
   "source": [
    "Now let's use our thetas to see what our encoding model says our spikes should look like. Normally, it would be good to make new predictions using stimulus you held out from training to see how well it generalizes, but for now we can just use our same design matrix. Take the dot product of your design matrix and your filter to generate the predicted spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d9592-0dda-4a7a-b2dd-820bf782911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here\n",
    "\n",
    "### Plot real spike train and prediction\n",
    "plt.clf()\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.stem(ttplot,spikes_binned[iiplot], linefmt='b-', basefmt='k-', label=\"spike ct\")\n",
    "### plot the predictions here in a different color\n",
    "plt.stem()\n",
    "plt.ylabel('spike count'); plt.xlabel('time (s)')\n",
    "plt.xlim([ttplot[0], ttplot[-1]])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaed76a-ede1-4c79-a331-160cc5eb61d2",
   "metadata": {},
   "source": [
    "Did our model do a good job? Do you think the neuron is acting as linear-gaussian filters of the stimulus? Maybe the neuron has a set mean firing rate that is modulated by the stimulus, rather than purely firing based on the stimulus alone. We can incorporate this into our model by adding a constant offset to our design matrix, essentially an extra variable that is always set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1e080-4183-442d-be4e-db49aeabf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code that adds a column of ones to your design matrix\n",
    "\n",
    "# compute the new STA\n",
    "\n",
    "# plot model prediction with and without the offset, along with the actual binned spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad734c04-25de-462f-80b2-e0752731b267",
   "metadata": {},
   "source": [
    "## Linear Non-linear Poisson Model\n",
    "\n",
    "Now lets fit a Linear Non-linear Poisson (LNP / Poisson GLM) model to this neuron. Unlike in the linear case, we don't have a closed form solution to find the optimal thetas, so we will have to manually perform MLE. Fortunately the statsmodels module in python supports various kinds of [GLMs](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels.genmod.generalized_linear_model.GLM), including Poisson GLMs with an exponential nonlinearity, which is what we will use. Keep in mind that the vocabulary statsmodels documentation uses is the \"link function\" which is the *inverse* of the nonlinearity. So to run an exponential GLM you want a logarithm link function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253bcbf-4712-4235-a4e7-158d00a82c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package available for download from\n",
    "# https://www.statsmodels.org/stable/install.html\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# insert code here to fit poisson GLM, be sure to use your design matrix with the offset\n",
    "\n",
    "# insert code here to obtain thetas\n",
    "\n",
    "# insert code here to generate predicted spikes\n",
    "\n",
    "# insert code here to plot LNP model predictions, along with linear gaussian model predictions, and raw spike rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6c895-a10d-4890-a7c6-7e5fc8ffd13b",
   "metadata": {},
   "source": [
    "Which looks best? Which has the best $R^2$ with the raw data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c5ed5-9e01-40fd-9efa-0ba74384c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here to compute R^2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
